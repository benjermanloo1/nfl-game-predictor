{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "566edcce-0e6a-4bfa-8f89-9f7b5fae9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c9138994-afd8-4830-9352-26a4411f6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time frame for scraping\n",
    "SEASONS = list(range(2016, 2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f6cb3dc3-64d8-4ecf-a9eb-50c3493a38d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directories to store data\n",
    "DATA_DIR = \"data\"\n",
    "SCHEDULES = os.path.join(DATA_DIR, \"schedules\")\n",
    "SCORES = os.path.join(DATA_DIR, \"scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d7f27ce7-b85f-47ea-88ff-5b3528714472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template url for schedule data\n",
    "url_start = \"https://www.pro-football-reference.com/years/{}/games.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "560e5768-5dc7-4012-8e7f-432812f5cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping schedule tables in time frame\n",
    "def scrape_season(season):\n",
    "    url = url_start.format(season)\n",
    "    data = requests.get(url)\n",
    "\n",
    "     # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    \n",
    "    # Find the specific section containing the schedule (table)\n",
    "    schedule = soup.find(id=\"div_games\")\n",
    "    \n",
    "    # Check if the table exists\n",
    "    if schedule:\n",
    "        html_snippet = schedule.prettify()\n",
    "\n",
    "        save_path = os.path.join(SCHEDULES, f\"{season}_schedule.html\")\n",
    "        with open(save_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_snippet)\n",
    "        print(f\"Schedule table for season {season} saved to {save_path}\")\n",
    "    else:\n",
    "        print(f\"Schedule table for season {season} not found.\")\n",
    "\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "798c36f2-e3ca-41c8-b4cc-2a5445494945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule table for season 2024 saved to data\\schedules\\2024_schedule.html\n"
     ]
    }
   ],
   "source": [
    "scrape_season(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "0c701312-781b-45ad-9eff-333881719dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "file_path = os.path.join(SCHEDULES, f\"{2024}_schedule.html\")\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    page = f.read()\n",
    "\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "# Find all <td> elements with class=\"center\" and data-stat=\"boxscore_word\"\n",
    "td_elements = soup.find_all('td', class_='center', attrs={'data-stat': 'boxscore_word'})\n",
    "            \n",
    "# Loop through each <td> element\n",
    "for td in td_elements:\n",
    "    # Find the <a> tag inside the <td>\n",
    "    boxscore_link = td.find('a')\n",
    "    if boxscore_link:\n",
    "        # Extract the href attribute\n",
    "        href = boxscore_link.get('href')\n",
    "        links.append(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5c40161a-19cf-4f80-98c4-96319cc95f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "67b9a168-15b6-4e0f-aaa1-75b175b2794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selenium_driver():\n",
    "    options = Options()\n",
    "    options.headless = True  # Run in headless mode\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "7fea97a6-02fb-4121-b132-cf5bca4761b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_box_scores(link):\n",
    "    url = box_score_url.format(link)\n",
    "    \n",
    "    # Use Selenium to open the page and wait for it to load completely\n",
    "    driver = get_selenium_driver()\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for a few seconds to allow content to load\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Parse the page with BeautifulSoup after it has fully loaded\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    # Finding and removing unwanted elements\n",
    "    soup.find(id=\"all_other_scores\").decompose()\n",
    "    soup.find(id=\"all_home_starters\").decompose()\n",
    "    soup.find(id=\"all_home_snap_counts\").decompose()\n",
    "    soup.find(id=\"all_home_drives\").decompose()\n",
    "    soup.find(id=\"all_vis_starters\").decompose()\n",
    "    soup.find(id=\"all_vis_snap_counts\").decompose()\n",
    "    soup.find(id=\"all_vis_drives\").decompose()\n",
    "    soup.find(id=\"all_pbp\").decompose()\n",
    "    soup.find(id=\"all_expected_points\").decompose()\n",
    "    soup.find(id=\"all_officials\").decompose()\n",
    "    soup.find(id=\"all_game_info\").decompose()\n",
    "    soup.find(id=\"all_scoring\").decompose()\n",
    "    soup.find(id=\"all_player_offense\").decompose()\n",
    "    soup.find(id=\"all_player_defense\").decompose()\n",
    "\n",
    "    # Find the content section\n",
    "    box_score = soup.find(id=\"content\")\n",
    "    \n",
    "    # Check if the box score exists and extract tables\n",
    "    if box_score:\n",
    "        html_snippet = box_score.prettify()\n",
    "        \n",
    "        match = re.search(r'boxscores/([^.]+)', url)\n",
    "        \n",
    "        save_path = os.path.join(SCORES, f\"{match.group(1)}_box_score.html\")\n",
    "        with open(save_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_snippet)\n",
    "    else:\n",
    "        print(\"Box score not found\")\n",
    "    \n",
    "    # Add delay between requests if scraping multiple pages\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b0190-bbd5-4f67-8752-272ffca4eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all box scores of the season\n",
    "for link in links:\n",
    "    scrape_box_scores(link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
